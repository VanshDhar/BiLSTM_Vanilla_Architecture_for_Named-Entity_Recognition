{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "import json\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading Vocabulary and Labels Json File\"\"\"\n",
    "with open('./vocab.json') as json_file:\n",
    "    word_idx = json.load(json_file)\n",
    "\n",
    "with open('./label.json') as json_file:\n",
    "    label_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14987 14987\n",
      "3466 3466\n",
      "3684\n"
     ]
    }
   ],
   "source": [
    "def prep_dataset(dataset):\n",
    "    train_x, train_y = list(), list()\n",
    "    x, y = list(), list()\n",
    "    first = 1\n",
    "    for row in dataset.itertuples():\n",
    "        # print(type(row.id))\n",
    "        # break\n",
    "        if(row.id == '1' and first == 0):\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "            x = list()\n",
    "            y = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "        y.append(row.NER)\n",
    "\n",
    "    train_x.append(x)\n",
    "    train_y.append(y)\n",
    "\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    train_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 2:\n",
    "                id, word, ner_tag = line.strip().split(\" \")\n",
    "                train_df.append([id, word, ner_tag])\n",
    "\n",
    "    train_df = pd.DataFrame(train_df, columns=['id', 'word', 'NER'])\n",
    "    train_df = train_df.dropna()\n",
    "    train_x, train_y = prep_dataset(train_df)\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def prep_dataset_test(dataset):\n",
    "    train_x = list()\n",
    "    x = list()\n",
    "    first = 1\n",
    "    for row in dataset.itertuples():\n",
    "        # print(type(row.id))\n",
    "        # break\n",
    "        if(row.id == '1' and first == 0):\n",
    "            train_x.append(x)\n",
    "            x = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "\n",
    "    train_x.append(x)\n",
    "    return train_x\n",
    "\n",
    "\n",
    "def read_file_test(path):\n",
    "    train_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 1:\n",
    "                id, word = line.strip().split(\" \")\n",
    "                train_df.append([id, word])\n",
    "\n",
    "    train_df = pd.DataFrame(train_df, columns=['id', 'word'])\n",
    "    train_df = train_df.dropna()\n",
    "    train_x = prep_dataset_test(train_df)\n",
    "    return train_x\n",
    "\n",
    "\n",
    "train_x, train_y = read_file('./data/train')\n",
    "val_x, val_y = read_file('./data/dev')\n",
    "test_x = read_file_test('./data/test')\n",
    "\n",
    "#print(len(train_x), len(train_y))\n",
    "#print(len(val_x), len(val_y))\n",
    "#print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 bidirectional, dropout_val, tag_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \"\"\" Hyper Parameters \"\"\"\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        \"\"\" Initializing Network \"\"\"\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim)  # Embedding Layer\n",
    "        self.embedding.weight.data.uniform_(-1,1)\n",
    "        self.LSTM = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions,\n",
    "                            linear_out_dim)  # 2 for bidirection\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU(alpha=0.01)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):  # sen_len\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class BiLSTM_DataLoader(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
    "        y_instance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
    "        return x_instance, y_instance\n",
    "\n",
    "class BiLSTM_TestLoader(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
    "        # y_instance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
    "        return x_instance\n",
    "\n",
    "\n",
    "class CustomCollator(object):\n",
    "\n",
    "    def __init__(self, vocab, label):\n",
    "        self.params = vocab\n",
    "        self.label = label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        (xx, yy) = zip(*batch)\n",
    "        x_len = [len(x) for x in xx]\n",
    "        y_len = [len(y) for y in yy]\n",
    "        batch_max_len = max([len(s) for s in xx])\n",
    "        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))\n",
    "        batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            cur_len = len(xx[j])\n",
    "            batch_data[j][:cur_len] = xx[j]\n",
    "            batch_labels[j][:cur_len] = yy[j]\n",
    "\n",
    "        batch_data, batch_labels = torch.LongTensor(\n",
    "            batch_data), torch.LongTensor(batch_labels)\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        return batch_data, batch_labels, x_len, y_len\n",
    "\n",
    "class CustomTestCollator(object):\n",
    "\n",
    "    def __init__(self, vocab, label):\n",
    "        self.params = vocab\n",
    "        self.label = label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        xx = batch\n",
    "        x_len = [len(x) for x in xx]\n",
    "        # y_len = [len(y) for y in yy]\n",
    "        batch_max_len = max([len(s) for s in xx])\n",
    "        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))\n",
    "        # batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            cur_len = len(xx[j])\n",
    "            batch_data[j][:cur_len] = xx[j]\n",
    "            # batch_labels[j][:cur_len] = yy[j]\n",
    "\n",
    "        batch_data = torch.LongTensor(batch_data)\n",
    "        batch_data = Variable(batch_data)\n",
    "\n",
    "        return batch_data, x_len\n",
    "\n",
    "\n",
    "\"\"\" Prepare Vocabualary\"\"\"\n",
    "def prep_vocab(dataset):\n",
    "\n",
    "    vocab = set()\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def prep_word_index(train_x, val_x, test_x):\n",
    "\n",
    "    word_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    idx = 2\n",
    "\n",
    "    for data in [train_x, val_x, test_x]:\n",
    "        for sent in data:\n",
    "            for word in sent:\n",
    "                if word not in word_idx:\n",
    "                    word_idx[word] = idx\n",
    "                    idx += 1\n",
    "    return word_idx\n",
    "\n",
    "\n",
    "def vectorizing_sent(train_x, word_idx):\n",
    "\n",
    "    train_x_vec = list()\n",
    "    tmp_x = list()\n",
    "    for words in train_x:\n",
    "        for word in words:\n",
    "            tmp_x.append(word_idx[word])\n",
    "        train_x_vec.append(tmp_x)\n",
    "        tmp_x = list()\n",
    "\n",
    "    return train_x_vec\n",
    "\n",
    "\n",
    "def prep_label_dict(train_y, val_y):\n",
    "\n",
    "    label1 = prep_vocab(train_y)\n",
    "    label2 = prep_vocab(val_y)\n",
    "    label = label1.union(label2)\n",
    "    label_tuples = []\n",
    "    counter = 0\n",
    "    for tags in label:\n",
    "        label_tuples.append((tags, counter))\n",
    "        counter += 1\n",
    "    label_dict = dict(label_tuples)\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def vectorizing_label(train_y, label_dict):\n",
    "\n",
    "    train_y_vec = list()\n",
    "    for tags in train_y:\n",
    "        tmp_yy = list()\n",
    "        for label in tags:\n",
    "            tmp_yy.append(label_dict[label])\n",
    "        train_y_vec.append(tmp_yy)\n",
    "    return train_y_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_vec = vectorizing_sent(train_x, word_idx)\n",
    "test_x_vec = vectorizing_sent(test_x, word_idx)\n",
    "val_x_vec = vectorizing_sent(val_x, word_idx)\n",
    "train_y_vec = vectorizing_label(train_y, label_dict)\n",
    "val_y_vec = vectorizing_label(val_y, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0200, 4.1500, 1.0000, 2.7300, 2.3000, 2.3600, 2.4600, 3.0000, 4.0900])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_class_weights(label_dict, train_y, val_y):\n",
    "    class_weights = dict()\n",
    "    for key in label_dict:\n",
    "        class_weights[key] = 0\n",
    "    total_nm_tags = 0\n",
    "    for data in [train_y, val_y]:\n",
    "        for tags in data:\n",
    "            for tag in tags:\n",
    "                total_nm_tags += 1\n",
    "                class_weights[tag] += 1\n",
    "\n",
    "    class_wt = list()\n",
    "    for key in class_weights.keys():\n",
    "        if class_weights[key]:\n",
    "            score = round(math.log(0.35*total_nm_tags / class_weights[key]), 2)\n",
    "            class_weights[key] = score if score > 1.0 else 1.0\n",
    "        else:\n",
    "            class_weights[key] = 1.0\n",
    "        class_wt.append(class_weights[key])\n",
    "    class_wt = torch.tensor(class_wt)\n",
    "    return class_wt\n",
    "\n",
    "\n",
    "class_wt = initialize_class_weights(label_dict, train_y, val_y)\n",
    "class_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(30292, 100)\n",
       "  (LSTM): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       "  (elu): ELU(alpha=0.01)\n",
       "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the trained bilstm model with generic embedding\n",
    "BiLSTM_model = BiLSTM(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      bidirectional=True,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict))\n",
    "\n",
    "BiLSTM_model.load_state_dict(torch.load(\"./blstm1.pt\"))#125#184#b2_143\n",
    "BiLSTM_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5942 phrases; found: 5651 phrases; correct: 4433.\n",
      "accuracy:  95.34%; precision:  78.45%; recall:  74.60%; FB1:  76.48\n",
      "              LOC: precision:  86.31%; recall:  82.36%; FB1:  84.29  1753\n",
      "             MISC: precision:  78.90%; recall:  77.44%; FB1:  78.16  905\n",
      "              ORG: precision:  72.13%; recall:  69.28%; FB1:  70.67  1288\n",
      "              PER: precision:  74.90%; recall:  69.33%; FB1:  72.00  1705\n"
     ]
    }
   ],
   "source": [
    "#Testing on Validation Dataset \n",
    "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collator)\n",
    "\n",
    "# Reverse vocab and label Dictionary                            \n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "\n",
    "file = open(\"dev1.out\", 'w')\n",
    "for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "    pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    label = label.detach().numpy()\n",
    "    dev_data = dev_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(label), -1))\n",
    "\n",
    "    for i in range(len(dev_data)):\n",
    "        for j in range(len(dev_data[i])):\n",
    "            if dev_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[dev_data[i][j]]\n",
    "                gold = rev_label_dict[label[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j+1), word, gold, op]))\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "file.close()\n",
    "#!perl conll03eval.txt < dev1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on Testing Dataset \n",
    "\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CustomTestCollator(word_idx, label_dict)\n",
    "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                drop_last=True,\n",
    "                                collate_fn=custom_test_collator)\n",
    "\n",
    "\n",
    "file = open(\"test1.out\", 'w')\n",
    "for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "    pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    test_data = test_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(test_data), -1))\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        for j in range(len(test_data[i])):\n",
    "            if test_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[test_data[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j+1), word, op]))\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "        file.write(\"\\n\")\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30292 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Loading Embedding Matrix\"\"\"\n",
    "emb_matrix = np.load('emb_matrix.npy')\n",
    "vocab_size = emb_matrix.shape[0]\n",
    "vector_size = emb_matrix.shape[1]\n",
    "print(vocab_size, vector_size)\n",
    "\n",
    "class BiLSTM_Glove(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 bidirectional, dropout_val, tag_size, emb_matrix):\n",
    "        super(BiLSTM_Glove, self).__init__()\n",
    "        \"\"\" Hyper Parameters \"\"\"\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        \"\"\" Initializing Network \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding Layer\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))\n",
    "        self.LSTM = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim)  # 2 for bidirection\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU(alpha=0.01)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):  # sen_len\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5942 phrases; found: 5944 phrases; correct: 5346.\n",
      "accuracy:  98.07%; precision:  89.94%; recall:  89.97%; FB1:  89.95\n",
      "              LOC: precision:  94.40%; recall:  94.45%; FB1:  94.42  1838\n",
      "             MISC: precision:  83.06%; recall:  82.43%; FB1:  82.74  915\n",
      "              ORG: precision:  84.51%; recall:  83.82%; FB1:  84.16  1330\n",
      "              PER: precision:  92.80%; recall:  93.76%; FB1:  93.28  1861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#loading the trained bilstm model with Glove embedding\n",
    "BiLSTM_model = BiLSTM_Glove(vocab_size=len(word_idx),\n",
    "                    embedding_dim=100,\n",
    "                    linear_out_dim=128,\n",
    "                    hidden_dim=256,\n",
    "                    lstm_layers=1,\n",
    "                    bidirectional=True,\n",
    "                    dropout_val=0.33,\n",
    "                    tag_size=len(label_dict),\n",
    "                    emb_matrix = emb_matrix)\n",
    "\n",
    "BiLSTM_model.load_state_dict(torch.load(\"./blstm2.pt\"))\n",
    "BiLSTM_model.to(device)\n",
    "\n",
    "#Testing on Validation Dataset \n",
    "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collator)\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "\n",
    "file = open(\"dev2.out\", 'w')\n",
    "for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "    pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    label = label.detach().numpy()\n",
    "    dev_data = dev_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(label), -1))\n",
    "\n",
    "    for i in range(len(dev_data)):\n",
    "        for j in range(len(dev_data[i])):\n",
    "            if dev_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[dev_data[i][j]]\n",
    "                gold = rev_label_dict[label[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j+1), word, gold, op]))\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "file.close()\n",
    "\n",
    "#!perl conll03eval.txt < dev2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on Test datasety\n",
    "\n",
    "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CustomTestCollator(word_idx, label_dict)\n",
    "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                drop_last=True,\n",
    "                                collate_fn=custom_test_collator)\n",
    "\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "file = open(\"test2.out\", 'w')\n",
    "for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "    pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    # label = label.detach().numpy()\n",
    "    test_data = test_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(test_data), -1))\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        for j in range(len(test_data[i])):\n",
    "            if test_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[test_data[i][j]]\n",
    "                # gold = rev_label_dict[label[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j + 1), word, op]))\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
