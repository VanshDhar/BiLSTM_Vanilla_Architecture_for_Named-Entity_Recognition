{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below, prep_dataset function returns 2 lists one containing all the words in the training dataset and the other containing all the corresponding NER tags, prep_dataset_test function returns only one list containing all the words in the test dataset. read_file and read_file_test use the corresponding functions mentioned above to convert the input data into required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(dataset):\n",
    "    train_x, train_y = list(), list()\n",
    "    x, y = list(), list()\n",
    "    first = 1\n",
    "    for row in dataset.itertuples():\n",
    "        # print(type(row.id))\n",
    "        # break\n",
    "        if(row.id == '1' and first == 0):\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "            x = list()\n",
    "            y = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "        y.append(row.NER)\n",
    "\n",
    "    train_x.append(x)\n",
    "    train_y.append(y)\n",
    "\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    train_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 2:\n",
    "                id, word, ner_tag = line.strip().split(\" \")\n",
    "                train_df.append([id, word, ner_tag])\n",
    "\n",
    "    train_df = pd.DataFrame(train_df, columns=['id', 'word', 'NER'])\n",
    "    train_df = train_df.dropna()\n",
    "    train_x, train_y = prep_dataset(train_df)\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def prep_dataset_test(dataset):\n",
    "    train_x = list()\n",
    "    x = list()\n",
    "    first = 1\n",
    "    for row in dataset.itertuples():\n",
    "        # print(type(row.id))\n",
    "        # break\n",
    "        if(row.id == '1' and first == 0):\n",
    "            train_x.append(x)\n",
    "            x = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "\n",
    "    train_x.append(x)\n",
    "    return train_x\n",
    "\n",
    "\n",
    "def read_file_test(path):\n",
    "    train_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 1:\n",
    "                id, word = line.strip().split(\" \")\n",
    "                train_df.append([id, word])\n",
    "\n",
    "    train_df = pd.DataFrame(train_df, columns=['id', 'word'])\n",
    "    train_df = train_df.dropna()\n",
    "    train_x = prep_dataset_test(train_df)\n",
    "    return train_x\n",
    "\n",
    "\n",
    "train_x, train_y = read_file('./data/train')\n",
    "val_x, val_y = read_file('./data/dev')\n",
    "test_x = read_file_test('./data/test')\n",
    "\n",
    "print(len(train_x), len(train_y))\n",
    "print(len(val_x), len(val_y))\n",
    "print(len(test_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we define a BiLSTM class which basically represents our Bidirectional-Lstm model(with generic embedding layer). The BiLSTM_glove class represents our Bidirectional-Lstm model with Glove model based embedding. The CustomCollator function is used to manipulate the input of each batch to the Bilstm model during training and validation, while the CustomTestCollator function is used to manipulate the input of each batch to the Bilstm model during testing(used for making all batch sentences of equal length by padding the short ones). BiLSTM_DataLoader class is used to feed the data to the model from training and validation dataset. BiLSTM_TestLoader class is used to feed the data to the model from testing dataset. The create_emb_matrix function creates a matrix based on the glove model dictionary provided so as to feed the Bilstm model this matrix.(Note: We handle the issue of glove model dictionary only containing lowercase words by making the embedding the titled word equal to its lowercase counterpart plus a small displacement value for each dimension of the embedding ). The prep_vocab function creates a set of all the unique words in the dataset. The prep_word_index function categorizes all the unique words of the dataset into different numeric values which can be understood by the Bilstm model. The vectorizing_sent function is used to convert the list of words into a list of list in which each sublist represents a sentence(consisting of its words) of the dataset. The vectorizing_label function is used to convert the list of NER tags into a list of list in which each sublist represents all the NER tags for a particaular sentence of the dataset. The prep_label_dict function is used to categorize all the unique NER tags into discrete numerical values that our BiLSTM model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 bidirectional, dropout_val, tag_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \"\"\" Hyper Parameters \"\"\"\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        \"\"\" Initializing Network \"\"\"\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim)  # Embedding Layer\n",
    "        self.embedding.weight.data.uniform_(-1,1)\n",
    "        self.LSTM = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions,\n",
    "                            linear_out_dim)  # 2 for bidirection\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU(alpha=0.01)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):  # sen_len\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred\n",
    "\n",
    "class BiLSTM_glove(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 bidirectional, dropout_val, tag_size, emb_matrix):\n",
    "        super(BiLSTM_glove, self).__init__()\n",
    "        \"\"\" Hyper Parameters \"\"\"\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        \"\"\" Initializing Network \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding Layer\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))\n",
    "        self.LSTM = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim)  # 2 for bidirection\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU(alpha=0.01)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):  # sen_len\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "class BiLSTM_DataLoader(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
    "        y_instance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
    "        return x_instance, y_instance\n",
    "\n",
    "class BiLSTM_TestLoader(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
    "        # y_instance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
    "        return x_instance\n",
    "\n",
    "\n",
    "class CustomCollator(object):\n",
    "\n",
    "    def __init__(self, vocab, label):\n",
    "        self.params = vocab\n",
    "        self.label = label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        (xx, yy) = zip(*batch)\n",
    "        x_len = [len(x) for x in xx]\n",
    "        y_len = [len(y) for y in yy]\n",
    "        batch_max_len = max([len(s) for s in xx])\n",
    "        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))\n",
    "        batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            cur_len = len(xx[j])\n",
    "            batch_data[j][:cur_len] = xx[j]\n",
    "            batch_labels[j][:cur_len] = yy[j]\n",
    "\n",
    "        batch_data, batch_labels = torch.LongTensor(\n",
    "            batch_data), torch.LongTensor(batch_labels)\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        return batch_data, batch_labels, x_len, y_len\n",
    "\n",
    "class CustomTestCollator(object):\n",
    "\n",
    "    def __init__(self, vocab, label):\n",
    "        self.params = vocab\n",
    "        self.label = label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        xx = batch\n",
    "        x_len = [len(x) for x in xx]\n",
    "        # y_len = [len(y) for y in yy]\n",
    "        batch_max_len = max([len(s) for s in xx])\n",
    "        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))\n",
    "        # batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            cur_len = len(xx[j])\n",
    "            batch_data[j][:cur_len] = xx[j]\n",
    "            # batch_labels[j][:cur_len] = yy[j]\n",
    "\n",
    "        batch_data = torch.LongTensor(batch_data)\n",
    "        batch_data = Variable(batch_data)\n",
    "\n",
    "        return batch_data, x_len\n",
    "\n",
    "def create_emb_matrix(word_idx, emb_dict, dimension):\n",
    "\n",
    "    emb_matrix = np.zeros((len(word_idx), dimension))\n",
    "    for word, idx in word_idx.items():\n",
    "        if word in emb_dict:\n",
    "            emb_matrix[idx] = emb_dict[word]\n",
    "        else:\n",
    "            if word.lower() in emb_dict:\n",
    "                emb_matrix[idx] = emb_dict[word.lower()] + 5e-3\n",
    "            else:\n",
    "                emb_matrix[idx] = emb_dict[\"<UNK>\"]\n",
    "\n",
    "    return emb_matrix\n",
    "    \n",
    "    \n",
    "\"\"\" Prepare Vocabulary\"\"\"\n",
    "def prep_vocab(dataset):\n",
    "\n",
    "    vocab = set()\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def prep_word_index(train_x, val_x, test_x):\n",
    "\n",
    "    word_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    idx = 2\n",
    "\n",
    "    for data in [train_x, val_x, test_x]:\n",
    "        for sent in data:\n",
    "            for word in sent:\n",
    "                if word not in word_idx:\n",
    "                    word_idx[word] = idx\n",
    "                    idx += 1\n",
    "    return word_idx\n",
    "\n",
    "\n",
    "def vectorizing_sent(train_x, word_idx):\n",
    "\n",
    "    train_x_vec = list()\n",
    "    tmp_x = list()\n",
    "    for words in train_x:\n",
    "        for word in words:\n",
    "            tmp_x.append(word_idx[word])\n",
    "        train_x_vec.append(tmp_x)\n",
    "        tmp_x = list()\n",
    "\n",
    "    return train_x_vec\n",
    "\n",
    "\n",
    "def prep_label_dict(train_y, val_y):\n",
    "\n",
    "    label1 = prep_vocab(train_y)\n",
    "    label2 = prep_vocab(val_y)\n",
    "    label = label1.union(label2)\n",
    "    label_tuples = []\n",
    "    counter = 0\n",
    "    for tags in label:\n",
    "        label_tuples.append((tags, counter))\n",
    "        counter += 1\n",
    "    label_dict = dict(label_tuples)\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def vectorizing_label(train_y, label_dict):\n",
    "\n",
    "    train_y_vec = list()\n",
    "    for tags in train_y:\n",
    "        tmp_yy = list()\n",
    "        for label in tags:\n",
    "            tmp_yy.append(label_dict[label])\n",
    "        train_y_vec.append(tmp_yy)\n",
    "    return train_y_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below, we just use the above fuctions to process the input and output data for training, testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = prep_word_index(train_x, val_x, test_x)\n",
    "train_x_vec = vectorizing_sent(train_x, word_idx)\n",
    "test_x_vec = vectorizing_sent(test_x, word_idx)\n",
    "val_x_vec = vectorizing_sent(val_x, word_idx)\n",
    "label_dict = prep_label_dict(train_y, val_y)\n",
    "train_y_vec = vectorizing_label(train_y, label_dict)\n",
    "val_y_vec = vectorizing_label(val_y, label_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we assign each NER tag with a weight based on the frequency of its appearance in the dataset so that we can use weighted loss for our BiLSTM model to prevent the model from overfitting on frequently appearing NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_class_weights(label_dict, train_y, val_y):\n",
    "    class_weights = dict()\n",
    "    for key in label_dict:\n",
    "        class_weights[key] = 0\n",
    "    total_nm_tags = 0\n",
    "    for data in [train_y, val_y]:\n",
    "        for tags in data:\n",
    "            for tag in tags:\n",
    "                total_nm_tags += 1\n",
    "                class_weights[tag] += 1\n",
    "\n",
    "    class_wt = list()\n",
    "    for key in class_weights.keys():\n",
    "        if class_weights[key]:\n",
    "            score = round(math.log(0.35*total_nm_tags / class_weights[key]), 2)\n",
    "            class_weights[key] = score if score > 1.0 else 1.0\n",
    "        else:\n",
    "            class_weights[key] = 1.0\n",
    "        class_wt.append(class_weights[key])\n",
    "    class_wt = torch.tensor(class_wt)\n",
    "    return class_wt\n",
    "\n",
    "\n",
    "class_wt = initialize_class_weights(label_dict, train_y, val_y)\n",
    "print(class_wt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we load our BiLSTM model with generic embedding layer, use all the custom funtions created for training, define loss function, optimizer and scheduler for our model and then train the model for 200 epochs. Simultaneously, also saving model after each epoch.\n",
    "## (Note Hyperparameters:\n",
    "##        Embedding dimension = 100\n",
    "##        Hidden dimension = 256\n",
    "##        Linear Output dimension = 128\n",
    "##        Bidirectional = True\n",
    "##        Dropout = 0.33\n",
    "##        Number of LSTM layers = 1\n",
    "##        Batch Size = 4\n",
    "##        Loss Function = Cross Entropy with class weights\n",
    "##        Optimizer = SGD with Learning Rate = 0.1 and Momentum = 0.9\n",
    "##        Epochs = 200\n",
    "## )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM_model = BiLSTM(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      bidirectional=True,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict))\n",
    "# BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_epoch_10.pt\"))\n",
    "BiLSTM_model.to(device)\n",
    "print(BiLSTM_model)\n",
    "\n",
    "BiLSTM_train = BiLSTM_DataLoader(train_x_vec, train_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader = DataLoader(dataset=BiLSTM_train,\n",
    "                        batch_size=4,\n",
    "                        drop_last=True,\n",
    "                        collate_fn=custom_collator)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
    "# criterion = nn.NLLLoss(weight=class_wt)\n",
    "# criterion = loss_fn\n",
    "criterion = criterion.to(device)\n",
    "criterion.requres_grad = True\n",
    "optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\")\n",
    "#scheduler = StepLR(optimizer, step_size=15, gamma=0.9)\n",
    "epochs = 200\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    train_loss = 0.0\n",
    "    # scheduler.step(train_loss)\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = BiLSTM_model(input.to(device), input_len)  # input_len\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input.size(1)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
    "    torch.save(BiLSTM_model.state_dict(),\n",
    "               'BiLSTM_epoch_' + str(i) + '.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we load the BiLSTM model saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM_model = BiLSTM(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      bidirectional=True,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict))\n",
    "\n",
    "BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_epoch_200.pt\"))#125#184#b2_143\n",
    "BiLSTM_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we make our model predicted the NER tags for the validation dataset and then store the predictions in a '.out' file in the required format\n",
    "## (Note: The best metrics in terms of performance achieved on validation dataset for my BiLSTM model with generic embedding were:\n",
    "## Precision: 79.41%, Recall: 75.36%, FB1: 77.33%\n",
    "## )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tesing on validation data\n",
    "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collator)\n",
    "\n",
    "# Reverse vocab and label Dictionary                            \n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "\n",
    "file = open(\"dev1_train.out\", 'w')\n",
    "for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "    pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    label = label.detach().numpy()\n",
    "    dev_data = dev_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(label), -1))\n",
    "\n",
    "    for i in range(len(dev_data)):\n",
    "        for j in range(len(dev_data[i])):\n",
    "            if dev_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[dev_data[i][j]]\n",
    "                gold = rev_label_dict[label[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j+1), word, gold, op]))\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "file.close()\n",
    "\n",
    "#!perl conll03eval.txt < op.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we make our model predicted the NER tags for the test dataset and then store the predictions in a '.out' file in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on Testing Dataset \n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CustomTestCollator(word_idx, label_dict)\n",
    "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                drop_last=True,\n",
    "                                collate_fn=custom_test_collator)\n",
    "\n",
    "\n",
    "file = open(\"test1_train.out\", 'w')\n",
    "for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "    pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    test_data = test_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(test_data), -1))\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        for j in range(len(test_data[i])):\n",
    "            if test_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[test_data[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j+1), word, op]))\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "        file.write(\"\\n\")\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below, we load the Glove model data and then convert it into a dictionary. We then add tokens \"\\<PAD>\" and \"\\<UNK>\" in the dictionary as well. and then convert the embedding dictionary into a matrix to feed to the embedding layer in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove = pd.read_csv('./glove.6B.100d.txt', sep=\" \",\n",
    "                    quoting=3, header=None, index_col=0)\n",
    "#glove_emb = {key: val.values for key, val in glove.T.items()}\n",
    "\n",
    "#word_idx = prep_word_index(train_x, val_x, test_x)\n",
    "#glove_vec = np.array([glove_emb[key] for key in glove_emb])\n",
    "#glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
    "#glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
    "#emb_matrix = create_emb_matrix(\n",
    "#    word_idx=word_idx, emb_dict=glove_emb, dimension=100)\n",
    "\n",
    "emb_matrix = np.load('emb_matrix.npy')\n",
    "\n",
    "vocab_size = emb_matrix.shape[0]\n",
    "vector_size = emb_matrix.shape[1]\n",
    "print(vocab_size, vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we load our BiLSTM model with glove embedding layer, and use all the custom funtions created for training, define loss function, optimizer and scheduler for our model and then train the model for 200 epochs. Simultaneously, also saving model after each epoch.\n",
    "\n",
    "## (Note Hyperparameters:\n",
    "##        Embedding dimension = 100\n",
    "##        Hidden dimension = 256\n",
    "##        Linear Output dimension = 128\n",
    "##        Bidirectional = True\n",
    "##        Dropout = 0.33\n",
    "##        Number of LSTM layers = 1\n",
    "##        Batch Size = 8\n",
    "##        Loss Function = Cross Entropy with class weights\n",
    "##        Optimizer = SGD with Learning Rate = 0.1 and Momentum = 0.9\n",
    "##        Epochs = 50\n",
    "## )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM_model = BiLSTM_glove(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      bidirectional=True,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict),\n",
    "                      emb_matrix=emb_matrix)\n",
    "# BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_glove_20.pt\"))\n",
    "BiLSTM_model.to(device)\n",
    "print(BiLSTM_model)\n",
    "\n",
    "BiLSTM_train = BiLSTM_DataLoader(train_x_vec, train_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader = DataLoader(dataset=BiLSTM_train,\n",
    "                        batch_size=8,\n",
    "                        drop_last=True,\n",
    "                        collate_fn=custom_collator)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
    "# criterion = nn.NLLLoss(weight=class_wt)\n",
    "# criterion = loss_fn\n",
    "criterion = criterion.to(device)\n",
    "criterion.requres_grad = True\n",
    "optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\")\n",
    "scheduler = StepLR(optimizer, step_size=15, gamma=0.9)\n",
    "epochs = 50\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    train_loss = 0.0\n",
    "    # scheduler.step(train_loss)\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = BiLSTM_model(input.to(device), input_len)  # input_len\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input.size(1)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
    "    torch.save(BiLSTM_model.state_dict(),\n",
    "               'BiLSTM_glove_' + str(i) + '.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we load the BiLSTM model with Glove embedding saved during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM_model = BiLSTM_glove(vocab_size=len(word_idx),\n",
    "                        embedding_dim=100,\n",
    "                        linear_out_dim=128,\n",
    "                        hidden_dim=256,\n",
    "                        lstm_layers=1,\n",
    "                        bidirectional = True,\n",
    "                        dropout_val=0.33,\n",
    "                        tag_size=len(label_dict),\n",
    "                        emb_matrix=emb_matrix)\n",
    "\n",
    "BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_glove_50.pt\"))\n",
    "BiLSTM_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we make our model predicted the NER tags for the validation dataset and then store the predictions in a '.out' file in the required format\n",
    "## (Note: The best metrics in terms of performance achieved on validation dataset for my BiLSTM model with Glove embedding were:\n",
    "## Precision: 89.94%, Recall: 89.97%, FB1: 89.95%\n",
    "## )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting for validation dataset\n",
    "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
    "                            batch_size=8,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collator)\n",
    "print(label_dict)\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "res = []\n",
    "file = open(\"dev2_train.out\", 'w')\n",
    "for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "    pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    label = label.detach().numpy()\n",
    "    dev_data = dev_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(label), -1))\n",
    "\n",
    "    for i in range(len(dev_data)):\n",
    "        for j in range(len(dev_data[i])):\n",
    "            if dev_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[dev_data[i][j]]\n",
    "                gold = rev_label_dict[label[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                res.append((word, gold, op))\n",
    "                file.write(\" \".join([str(j + 1), word, gold, op]))\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!perl conll03eval.txt < op.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we make our model predicted the NER tags for the test dataset and then store the predictions in a '.out' file in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting for testing dataset\n",
    "\n",
    "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CustomTestCollator(word_idx, label_dict)\n",
    "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                drop_last=True,\n",
    "                                collate_fn=custom_test_collator)\n",
    "\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "res = []\n",
    "file = open(\"test2_train.out\", 'w')\n",
    "for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "    pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    # label = label.detach().numpy()\n",
    "    test_data = test_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(test_data), -1))\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        for j in range(len(test_data[i])):\n",
    "            if test_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[test_data[i][j]]\n",
    "                # gold = rev_label_dict[label[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                res.append((word, op))\n",
    "                file.write(\" \".join([str(j + 1), word, op]))\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "file.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce271b99db1b250d22cac146c6eb9bdc2a3b9e3f607d82c95bc1468f582e9c43"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
